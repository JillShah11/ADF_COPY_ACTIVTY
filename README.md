# ğŸ“¦ ADF_COPY_ACTIVITY

## ğŸ“˜ Azure Data Factory - Copy Activity with Medallion Architecture

### ğŸ§¾ Overview

This project demonstrates how to use **Azure Data Factory (ADF)** to ingest data from GitHub repositories into **Azure Data Lake Storage (ADLS)**, following the **Medallion Architecture** (Bronze â†’ Silver â†’ Gold). The data is incrementally loaded and refined across layers for analytical and reporting purposes.

---

## ğŸ—ï¸ Project Architecture

### ğŸ”„ Data Flow

1. **Source**  
   - Public data from GitHub repositories (e.g., JSON files).

2. **Destination**  
   - Data is initially ingested into the **Bronze Layer** (landing zone) in ADLS.

3. **ADF Pipeline Components**
   - **Linked Services**: Connects ADF to GitHub and ADLS.
   - **ForEach Activity**: Dynamically loops through multiple datasets.
   - **Copy Activity**: Moves data from GitHub to ADLS (Bronze layer).

---

## ğŸ”§ Components

### 1. ğŸ”— Linked Services
- **GitHub Linked Service**  
  - Authenticated connection to fetch data from public GitHub repositories.
  
- **ADLS Linked Service**  
  - Secure connection to Azure Data Lake Storage for landing the data.

### 2. ğŸ“ Datasets
- **Source Dataset**  
  - Points to GitHub JSON files.
  
- **Sink Dataset**  
  - Configured to write data to appropriate locations in ADLS.

### 3. ğŸ› ï¸ Pipeline Configuration
- **ForEach Activity**  
  - Iterates over a list of repositories or datasets for dynamic data loading.
  
- **Copy Activity**  
  - Executes the actual data movement from GitHub to ADLS within the loop.
  
- **Incremental Loading**  
  - Configured where applicable to avoid reprocessing and improve performance.

### 4. ğŸ›ï¸ Medallion Architecture Implementation
- **Bronze Layer (Raw Data)**  
  - Ingested data directly from GitHub; minimally processed.

- **Silver Layer (Refined Data)**  
  - Cleaned and structured data for analytical readiness.

- **Gold Layer (Curated Data)**  
  - Business-ready data used for reporting and dashboarding (e.g., Power BI).

---

## ğŸŒŸ Key Features

âœ… Dynamic ingestion of multiple datasets via `ForEach` activity  
âœ… Secure and reusable connections using Linked Services  
âœ… Incremental load for optimized performance  
âœ… Follows **Medallion Architecture** best practices for scalable and clean data transformation

---

## âœ… Prerequisites

- ğŸ”§ Azure Data Factory instance  
- ğŸ—„ï¸ Azure Data Lake Storage (ADLS) account  
- ğŸŒ Access to desired GitHub repositories  
- ğŸ” Azure Key Vault (for securely managing credentials and secrets)

---

## ğŸ“¬ Contact

For questions or collaboration, feel free to reach out at:  
ğŸ“§ *[Your Contact Email]*
